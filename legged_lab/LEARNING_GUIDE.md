# Legged Lab æŠ€æœ¯æ·±åº¦å­¦ä¹ è®¡åˆ’

## ğŸ¯ æŠ€æœ¯æŒæ¡ç›®æ ‡

### é¢è¯•ä¸­å¯èƒ½é‡åˆ°çš„é—®é¢˜ç±»å‹

#### 1. å¼ºåŒ–å­¦ä¹ åŸºç¡€é—®é¢˜
**é¢è¯•å®˜é—®ï¼š"ä¸ºä»€ä¹ˆé€‰æ‹©PPOç®—æ³•ï¼Ÿ"**
**ä½ çš„å›ç­”åº”è¯¥åŒ…å«ï¼š**
- PPOæ˜¯ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œé€‚åˆè¿ç»­åŠ¨ä½œç©ºé—´
- å…·æœ‰é‡è¦æ€§é‡‡æ ·å’Œå‰ªåˆ‡æœºåˆ¶ï¼Œè®­ç»ƒç¨³å®š
- åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€
- ç›¸æ¯”TRPOè®¡ç®—æ›´é«˜æ•ˆ

#### 2. å¥–åŠ±è®¾è®¡é—®é¢˜
**é¢è¯•å®˜é—®ï¼š"å¦‚ä½•è®¾è®¡å¥–åŠ±å‡½æ•°è®©æœºå™¨äººå­¦ä¼šèµ°è·¯ï¼Ÿ"**
**ä½ çš„å›ç­”åº”è¯¥å±•ç¤ºï¼š**
```python
# å¥–åŠ±å‡½æ•°è®¾è®¡åŸåˆ™
reward_components = {
    "tracking": "è·Ÿè¸ªç›®æ ‡é€Ÿåº¦",
    "stability": "ä¿æŒèº«ä½“ç¨³å®š",
    "energy": "æœ€å°åŒ–èƒ½è€—",
    "safety": "é¿å…å±é™©è¡Œä¸º",
    "shaping": "å¼•å¯¼å­¦ä¹ è¿‡ç¨‹"
}

# å…·ä½“å®ç°ä¾‹å­
def track_lin_vel_xy_exp(env, std=0.5):
    """é€Ÿåº¦è·Ÿè¸ªå¥–åŠ± - æŒ‡æ•°å½¢å¼æ›´smooth"""
    vel_error = torch.norm(cmd_vel - actual_vel, dim=1)
    return torch.exp(-vel_error / std**2)
```

#### 3. æŠ€æœ¯å®ç°é—®é¢˜
**é¢è¯•å®˜é—®ï¼š"å¦‚ä½•å¤„ç†æœºå™¨äººçš„åŠ¨ä½œå»¶è¿Ÿï¼Ÿ"**
**ä½ çš„å›ç­”åº”è¯¥æåˆ°ï¼š**
```python
# åŠ¨ä½œå»¶è¿Ÿç¼“å†²å™¨
self.action_buffer = DelayBuffer(
    max_delay=self.cfg.domain_rand.action_delay.params["max_delay"],
    num_envs=self.num_envs,
    device=self.device
)
# æ¨¡æ‹ŸçœŸå®æœºå™¨äººçš„é€šä¿¡å»¶è¿Ÿ
delayed_action = self.action_buffer.compute(action)
```

## ğŸ“– æ·±åº¦å­¦ä¹ è·¯çº¿

### Week 1-2: å¼ºåŒ–å­¦ä¹ ç†è®º
**å¿…è¯»ææ–™ï¼š**
1. PPOè®ºæ–‡ï¼šProximal Policy Optimization Algorithms
2. Isaac Labæ–‡æ¡£ï¼šEnvironment Design Principles
3. RSL-RLåº“ï¼šPolicy Implementation

**å®è·µä»»åŠ¡ï¼š**
```bash
# 1. è·‘é€šåŸºç¡€è®­ç»ƒæµç¨‹
python scripts/train.py --task go2_flat --num_envs 512

# 2. åˆ†æè®­ç»ƒæ—¥å¿—
# æŸ¥çœ‹ wandb æˆ– tensorboard è¾“å‡º
# ç†è§£å„é¡¹å¥–åŠ±çš„å˜åŒ–æ›²çº¿

# 3. ä¿®æ”¹ç®€å•é…ç½®
# æ”¹å˜ç¯å¢ƒæ•°é‡ã€å­¦ä¹ ç‡ç­‰å‚æ•°
# è§‚å¯Ÿå¯¹è®­ç»ƒçš„å½±å“
```

### Week 3: ç¯å¢ƒè®¾è®¡æ·±å…¥
**æ ¸å¿ƒæ–‡ä»¶åˆ†æï¼š**
```python
# base_env.py - ç¯å¢ƒä¸»é€»è¾‘
class BaseEnv(VecEnv):
    def step(self, actions):
        # 1. åŠ¨ä½œå¤„ç†å’Œå»¶è¿Ÿæ¨¡æ‹Ÿ
        # 2. ç‰©ç†ä»¿çœŸæ­¥è¿›
        # 3. è§‚å¯Ÿè·å–
        # 4. å¥–åŠ±è®¡ç®—
        # 5. ç»ˆæ­¢æ¡ä»¶æ£€æŸ¥
        pass
    
    def get_observations(self):
        # ä¼ æ„Ÿå™¨æ•°æ®èåˆ
        # å†å²ä¿¡æ¯ç¼“å­˜
        # å™ªå£°æ·»åŠ 
        pass
```

**æ·±å…¥ç†è§£ç‚¹ï¼š**
1. **è§‚å¯Ÿç©ºé—´è®¾è®¡**ï¼šå…³èŠ‚è§’åº¦ã€é€Ÿåº¦ã€IMUã€åœ°å½¢æ‰«æ
2. **åŠ¨ä½œç©ºé—´è®¾è®¡**ï¼šå…³èŠ‚ä½ç½®ç›®æ ‡ã€PDæ§åˆ¶å‚æ•°
3. **ç‰©ç†ä»¿çœŸ**ï¼šPhysXå¼•æ“ã€æ¥è§¦æ£€æµ‹ã€ç¢°æ’å¤„ç†

### Week 4: ç®—æ³•ä¸ä¼˜åŒ–
**RSL-RLåº“æ·±å…¥ï¼š**
```python
# ç­–ç•¥ç½‘ç»œç»“æ„
class ActorCritic:
    def __init__(self):
        self.actor = MLP([obs_dim, 256, 256, action_dim])
        self.critic = MLP([obs_dim, 256, 256, 1])
    
    def act(self, obs):
        # ç­–ç•¥è¾“å‡ºåŠ¨ä½œåˆ†å¸ƒ
        return self.actor(obs)
    
    def evaluate(self, obs):
        # ä»·å€¼å‡½æ•°ä¼°è®¡
        return self.critic(obs)
```

## ğŸ” ä»£ç èµ°è¯»æ¸…å•

### å¿…é¡»ç†è§£çš„æ ¸å¿ƒå‡½æ•°

#### 1. ç¯å¢ƒæ ¸å¿ƒå¾ªç¯
```python
# base_env.py:step()
def step(self, actions: torch.Tensor):
    # åŠ¨ä½œå¤„ç†
    delayed_actions = self.action_buffer.compute(actions)
    
    # åº”ç”¨åŠ¨ä½œåˆ°æœºå™¨äºº
    self.robot.set_joint_position_target(delayed_actions)
    
    # ç‰©ç†ä»¿çœŸæ­¥è¿›
    for _ in range(self.cfg.sim.decimation):
        self.scene.write_data_to_sim()
        self.sim.step()
        self.scene.update(dt=self.physics_dt)
    
    # è·å–è§‚å¯Ÿ
    obs = self.get_observations()
    
    # è®¡ç®—å¥–åŠ±
    rewards = self.reward_manager.compute()
    
    # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
    dones = self.get_dones()
    
    return obs, rewards, dones, self.extras
```

#### 2. å¥–åŠ±è®¡ç®—æœºåˆ¶
```python
# rewards.py ä¸­çš„å…³é”®å‡½æ•°
def track_lin_vel_xy_yaw_frame_exp(env, std: float):
    """é€Ÿåº¦è·Ÿè¸ªå¥–åŠ±"""
    asset = env.scene["robot"]
    
    # è·å–æœºå™¨äººå½“å‰é€Ÿåº¦ï¼ˆyawåæ ‡ç³»ï¼‰
    vel_yaw = quat_apply_inverse(
        yaw_quat(asset.data.root_quat_w), 
        asset.data.root_lin_vel_w[:, :3]
    )
    
    # è®¡ç®—ä¸å‘½ä»¤é€Ÿåº¦çš„è¯¯å·®
    vel_error = torch.sum(
        torch.square(env.command_generator.command[:, :2] - vel_yaw[:, :2]), 
        dim=1
    )
    
    # æŒ‡æ•°å½¢å¼çš„å¥–åŠ±ï¼ˆæ›´å¹³æ»‘ï¼‰
    return torch.exp(-vel_error / std**2)
```

#### 3. åœ°å½¢ç”Ÿæˆç³»ç»Ÿ
```python
# terrain_generator_cfg.py
ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
    curriculum=True,  # å¯ç”¨è¯¾ç¨‹å­¦ä¹ 
    size=(8.0, 8.0),  # åœ°å½¢å¤§å°
    sub_terrains={
        "pyramid_stairs_28": MeshInvertedPyramidStairsTerrainCfg(
            proportion=0.1,  # å æ€»åœ°å½¢çš„æ¯”ä¾‹
            step_height_range=(0.0, 0.23),  # å°é˜¶é«˜åº¦èŒƒå›´
            step_width=0.28,  # å°é˜¶å®½åº¦
        ),
        # ... å…¶ä»–7ç§åœ°å½¢ç±»å‹
    }
)
```

## ğŸª å®æˆ˜ç»ƒä¹ æ¸…å•

### ç»ƒä¹ 1ï¼šå¥–åŠ±å‡½æ•°è°ƒè¯•
```python
# ä»»åŠ¡ï¼šä¿®æ”¹GO2çš„å¥–åŠ±æƒé‡ï¼Œè®©å®ƒå­¦ä¼šæ›´å¥½çš„æ­¥æ€
# æ–‡ä»¶ï¼šenvs/go2/go2_config.py

class Go2RewardCfg(RewardCfg):
    # å®éªŒï¼šè°ƒæ•´è¿™äº›æƒé‡
    track_lin_vel_xy_exp = RewTerm(weight=2.0)  # åŸæ¥3.0
    feet_air_time = RewTerm(weight=0.2)         # åŸæ¥0.1
    upward = RewTerm(weight=2.0)                # åŸæ¥1.0
```

### ç»ƒä¹ 2ï¼šæ·»åŠ æ–°çš„å¥–åŠ±é¡¹
```python
# ä»»åŠ¡ï¼šåœ¨rewards.pyä¸­æ·»åŠ èƒ½è€—æœ€å°åŒ–å¥–åŠ±
def energy_minimization(env, asset_cfg=SceneEntityCfg("robot")):
    """æœ€å°åŒ–å…³èŠ‚åŠŸç‡æ¶ˆè€—"""
    asset = env.scene[asset_cfg.name]
    torques = asset.data.applied_torque
    joint_vel = asset.data.joint_vel
    power = torch.abs(torques * joint_vel)
    return -torch.sum(power, dim=1)
```

### ç»ƒä¹ 3ï¼šåœ°å½¢è‡ªå®šä¹‰
```python
# ä»»åŠ¡ï¼šåˆ›å»ºæ–°çš„åœ°å½¢ç±»å‹
"custom_obstacles": terrain_gen.MeshRandomGridTerrainCfg(
    proportion=0.1,
    grid_width=0.3,
    grid_height_range=(0.05, 0.2),
    platform_width=1.5
)
```

## ğŸ“ é¢è¯•å‡†å¤‡æ¸…å•

### æŠ€æœ¯é—®é¢˜å‡†å¤‡

#### å¼ºåŒ–å­¦ä¹ ç±»
1. "è§£é‡ŠPPOç®—æ³•çš„æ ¸å¿ƒæ€æƒ³"
2. "ä¸ºä»€ä¹ˆåœ¨æœºå™¨äººæ§åˆ¶ä¸­ä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´ï¼Ÿ"
3. "å¦‚ä½•è®¾è®¡ç¨€ç–å¥–åŠ±vså¯†é›†å¥–åŠ±ï¼Ÿ"
4. "ä»€ä¹ˆæ˜¯å€¼å‡½æ•°ï¼Ÿå®ƒåœ¨è®­ç»ƒä¸­çš„ä½œç”¨ï¼Ÿ"

#### æœºå™¨äººæ§åˆ¶ç±»  
1. "å¦‚ä½•å¤„ç†æœºå™¨äººåŠ¨åŠ›å­¦ï¼Ÿ"
2. "PDæ§åˆ¶åœ¨è¿™ä¸ªé¡¹ç›®ä¸­çš„ä½œç”¨ï¼Ÿ"
3. "å¦‚ä½•ä¿è¯è®­ç»ƒçš„å®‰å…¨æ€§ï¼Ÿ"
4. "åŸŸéšæœºåŒ–çš„å¿…è¦æ€§ï¼Ÿ"

#### ç³»ç»Ÿè®¾è®¡ç±»
1. "å¦‚ä½•æ‰©å±•åˆ°æ–°çš„æœºå™¨äººï¼Ÿ"
2. "å¹¶è¡Œç¯å¢ƒçš„å¥½å¤„å’ŒæŒ‘æˆ˜ï¼Ÿ"
3. "ä»¿çœŸåˆ°çœŸå®çš„å·®è·å¦‚ä½•å¤„ç†ï¼Ÿ"
4. "æ€§èƒ½ä¼˜åŒ–çš„å…³é”®ç‚¹ï¼Ÿ"

### é¡¹ç›®å±•ç¤ºå‡†å¤‡
```python
# å‡†å¤‡demoè„šæœ¬
python scripts/play.py --task go2_rough --checkpoint best_model.pth --num_envs 64
```

## ğŸš€ è¿›é˜¶æ‹“å±•

### é«˜çº§è¯é¢˜
1. **è¯¾ç¨‹å­¦ä¹ **ï¼šåœ°å½¢éš¾åº¦è‡ªåŠ¨è°ƒèŠ‚æœºåˆ¶
2. **åŸŸéšæœºåŒ–**ï¼šæé«˜sim-to-realè½¬æ¢
3. **å¤šæ¨¡æ€æ§åˆ¶**ï¼šç»“åˆè§†è§‰å’Œæœ¬ä½“æ„Ÿè§‰
4. **åˆ†å±‚å¼ºåŒ–å­¦ä¹ **ï¼šé«˜å±‚è§„åˆ’+ä½å±‚æ§åˆ¶

### ç›¸å…³è®ºæ–‡é˜…è¯»
1. Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning
2. Learning Agile Robotic Locomotion Skills by Imitating Animals  
3. RMA: Rapid Motor Adaptation for Legged Robots
4. Learning Robust, Real-Time, Reactive Robotic Movement
